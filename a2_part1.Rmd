---
title: "Assignment 2 - Meta-analysis of pitch in schizophrenia"
author: "Riccardo Fusaroli"
date: "16/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages(pacman)
library(pacman)
pacman::p_load(tidyverse, brms, tidybayes, conflicted, msm, readxl)

conflict_scout() #checking any possible conflicts between packages
conflict_prefer('ar', 'brms')
conflict_prefer('filter', 'dplyr')
conflict_prefer('lag', 'dplyr') #choosing the packages to prefer if conflict arises
```

## Question 1

```{r}
# arabic digits - simulation
# ---> model fitting
# 1. Ground truth level: the real underlying truth  - norm(0.4,0.4)
# 2. Sample level(Study level: 100 samples(studies) - norm(mu drawn from the above, sd drawn from the above)
# 3. Participant level: tnorm(20, 10, 10) participants in each study
# 4. Results level: estimates of regressions based on the participants
#        -->(I) Fit no. 1 Bayes model: Given the study results what are the estimated true parameters???
# 5. Published results level: include publication bias filter
#        -->(II) Fit no. 2 Bayes model: Given the **publilshed** results what are the estimated true parameters???
#       --> Compare the 2 models


# Questions:
#   - what happens with sigma? (divergance plots)?
#   - what does 'nr' or "\r\n \r\n" in education_sz__sd mean?

#to_do:
#     - (done) does he model the pitch for 2 healthy and schizophrenic participants (how if only one distribution) or just the difference (how if you have to model individual participants as well)?
          #answer: the slides show he just models the difference
#     - really dumb you have to check stuff like that xd but check whether it really means significance
#     - (optional) maybe it would be nice to put the type of the bias and the values as function arguments
#     - **check what would removing the outliers do to model estimates!**


```
```{r}
#(done) 1. Simulate data 
#     to setup the analysis and gain insight on the structure of the problem. Simulate         one dataset of 100 studies (n of participants should follow a normal distribution        with mean of 20, sd of 10, but no fewer than 10 participants), with a mean effect        size of 0.4, average deviation by study of .4 and measurement error of .8. The data       you get should have one row per study, with an effect size mean and standard error.
#      Then simulate publication bias (only some of the studies you simulate are likely #        to be published, which?)
# (done) 2.Build a proper bayesian model to analyze the simulated data.
# (done) 3. re-run the model on published studies, assess the difference, 
# 4. discuss what this implies for your model
# (done) 5. remember to use at least one plot to visualize your results. 
#6. BONUS question: do a power/precision analysis: w this kind of sample sizes ( participants) how many studies would you need to acquire good precision (e.g. .1 sd in the pop level estimate)
```
```{r}
simulate_data <- function(n = 100, gt_mean = 0.4, gt_sd = 0.4, error = 0.8, seed = 1)
  #expaning the names: n - number of studies, gt stands for 'ground-truth'
  {
  set.seed(seed)
    
  data <- tibble(study = seq(1, n, by = 1)) %>% 
    rowwise %>% 
    mutate(t_effect = rnorm(1, gt_mean, gt_sd) %>% round(2), 
           sample_size = rtnorm(1, 20, 10, lower = 10) %>% round,
           ind_effects = list(rnorm(sample_size, mean = t_effect, sd = error) %>% round(2)),
           effect = mean(ind_effects),
           effect_sigma = sd(ind_effects) / sqrt(sample_size), 
           ci_lower = effect - 1.96 * effect_sigma, # just so it's easier to plot later
           ci_upper = effect + 1.96 * effect_sigma, 
           signif = if_else(abs(effect) - 1.96 * effect_sigma > 0, 'yes' , 'no'),
           pub = if_else(signif == 'yes' & effect > 0, rbinom(1, 1, 0.9), rbinom(1, 1, 0.1))) %>% 
    ungroup %>% 
    relocate(c(t_effect, sample_size, ind_effects), .after = pub)

}
```
### Checking if all worked fine
```{r}
check <- simulate_data(n = 10000)

check %>% summarise(mean(t_effect), sd(t_effect))


n_tot <- check %>%
  count(signif == 'yes' & effect > 0) %>%
  pull(n)
check %>% filter(pub == 1) %>%
  count(signif == 'yes' & effect > 0, pub) %>%
  mutate(pct = n / n_tot)

rm(n_tot, check)
```


```{r}
data <- simulate_data()
data_pub <- data %>% 
  filter(pub == 1)

effect_hist <- function(data){
  ggplot(data, aes(x = effect)) +
    geom_histogram(binwidth = 0.1, fill = 'darkgreen', color = 'black', alpha = 0.1) +
    geom_vline(aes(xintercept = 0.4, color = 'real effect size'),
               linetype = 'dashed', size = 0.6) +
    geom_vline(aes(xintercept = mean(effect), color = 'calculated mean effect size'),
               linetype = 'dashed', size = 0.6) +
    scale_x_continuous(n.breaks = 8) +
    labs(x = 'Effect size',
         y = 'Count') +
    scale_color_manual(name = element_blank(), values = c(`real effect size` = "black", `calculated mean effect size` = "darkgreen")) +
    theme_minimal()
}

effect_hist(data) + ggtitle("All studies")
effect_hist(data_pub) + ggtitle("Published studies")

```


```{r}
rbind(data %>% mutate(index = 'all'),
      data_pub %>% mutate(index = 'published')) %>% 
ggplot(aes(x = index, y = effect, fill = index)) + 
  geom_boxplot(alpha = 0.3) +
  theme_minimal() +
  guides(alpha = 'none') +
  scale_fill_manual(values = c('navy', 'darkgreen'), 
                    labels = c('all studies', 'published studies'), 
                    name = element_blank()) +
  ylab('Effect size') +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'top')
```

```{r}
funnel_base <- function(data, n_small_big_cutoff = 30, null = 0){
  effect_mean <-  mean(data$effect)

  line_data = tibble(
    se_line =  seq(0, max(data$effect_sigma), by = 0.001),
    
    line_u95 =  effect_mean + 1.96*se_line,
    line_l95 =  effect_mean - 1.96*se_line,
    
    line_u99 =  effect_mean + 3.29*se_line,
    line_l99 =  effect_mean -3.29*se_line)

  data %>% mutate(`Sample size` = if_else(sample_size < n_small_big_cutoff, 'small', 'big')) %>% 
    ggplot(aes(x = effect, y = effect_sigma)) +
      scale_y_reverse() +
      geom_point(aes(shape = `Sample size` )) +
      geom_line(aes(x = line_u95, y = se_line), linetype = 'dashed', data = line_data) +
      geom_line(aes(x = line_l95, y = se_line), linetype = 'dashed', data = line_data) +
      geom_line(aes(x = line_u99, y = se_line), linetype = 'dotted', data = line_data) +
      geom_line(aes(x = line_l99, y = se_line), linetype = 'dotted', data = line_data) +
      geom_segment(aes(x = null, y = 0, xend = null, yend = max(effect_sigma), colour = 'Null hypothesis'),
                 linetype = 'dashed',
                 size = 0.6,
                 alpha = 0.1) +
      theme_minimal() +
      scale_shape_manual(values = c('circle', 'circle open')) +
      scale_color_manual(values = c(`Null hypothesis` = 'darkred')) +
      labs(x = 'Effect size',
           y = 'Standard Error')
}
```



```{r}
map(.x = list(data, data_pub), .f = function(.x){
  observed_mean <- mean(.x$effect)
  true_mean <- 0.4
  
  funnel_base(.x) +
    geom_segment(aes(x = observed_mean, 
                     y = 0, 
                     xend = observed_mean, 
                     yend = max(effect_sigma),  
                     colour = 'Mean sample effect size'),
                 linetype = 'solid',
                 size = 1) +
    geom_segment(aes(x = true_mean, 
                     y = 0, 
                     xend = true_mean, 
                     yend = max(effect_sigma),
                     colour = 'True parameter value'),
                 linetype = 'dashed',
                 size = 1)
    scale_colour_manual(values = c(`Mean sample effect size` = 'black', 
                                   `True parameter value` = 'grey',
                                   `Null hypothesis` = 'darkred'))
})
```
### Defining the formula and priors
```{r}
f <- bf(effect | se(effect_sigma) ~ 1 + (1|study))

get_prior(f, data)

```
```{r}
priors <- c(prior(normal(0, 0.5), class = Intercept),
            prior(normal(0, 0.6), class = sd))
```
```{r}
prior_m_all <- brm(f,
                   data,
                   family = gaussian,
                   prior = priors,
                   sample_prior = 'only',
                   backend = 'cmdstanr',
                   cores = 3
                   )
```

```{r}
pp_check(prior_m_all, ndraws = 100)

summary(prior_m_all)
```
### Fitting the models
```{r}
m_all <- brm(f, 
             data,
             family = gaussian,
             prior = priors,
             sample_prior = T,
             backend = 'cmdstanr',
             cores = 3
             )

m_pub <- update(m_all, 
                newdata = data %>% filter(pub == 1))
```
```{r}
pp_check(m_all, ndraws = 100)+
  ggtitle("Model trained on all of the studies")

pp_check(m_pub, ndraws = 100)+
  ggtitle("Model trained on published studies")
```
```{r}
summary(m_all)
summary(m_pub)
```

### Convergance tests
```{r}
models <- list(m_all, m_pub)
      
# launch_shinystan(f_m) # - very nice for exploring and diagnosing the model, but opens up in a new window

map(.x = models, ~ mcmc_plot(.x, type = 'trace') + 
    theme_classic() + 
    scale_color_manual(values=c("#E66101", "#998EC3", "#542788", "#F1A340")) + 
    ylab("") + 
    xlab("Iteration") + 
    labs(subtitle = 'Trace Plots'))

map(.x = models, ~ mcmc_plot(.x, type = 'rhat_hist'))
map(.x = models, ~ mcmc_plot(.x, type = 'neff'))
```
### Posterior-prior checks

```{r}
get_variables(m_all)


```

```{r}
pp_update_plot <- function(model){

bind_rows(
  gather_draws(model, c(b_Intercept, sd_study__Intercept)) %>%
    mutate(index = 'posterior') %>% 
    mutate(.variable = if_else(.variable == 'b_Intercept', 'Intercept', 'SD')),
  gather_draws(model, `prior.*`, regex = T) %>% 
    mutate(index = 'prior') %>% 
    mutate(.variable = if_else(.variable == 'prior_Intercept', 'Intercept', 'SD'))
  ) %>%
    ggplot(aes(x = .value, fill = index, alpha = 0.3)) +
      geom_density() +
    facet_grid(~ .variable) +
    theme_minimal() +
    guides(alpha = 'none') +
    scale_fill_manual(name = element_blank(), values = c('red', 'steelblue')) +
    labs(x = element_blank())
} 

pp_update_plot(m_all)
pp_update_plot(m_pub)
```
### Comparing estimated and true values:


```{r}
bind_rows(gather_draws(m_all, c(b_Intercept, sd_study__Intercept)) %>% mean_qi, 
      gather_draws(m_pub, c(b_Intercept, sd_study__Intercept)) %>% mean_qi) %>% 
  mutate(t_value = rep(c(0.4, 0.4), times = 2),
         model = rep(c("all", "pub"), each = 2)) %>% 
  
  ggplot(aes(x = model, y = .value, ymin = .lower, ymax = .upper, color = model)) +
    geom_hline(aes(yintercept = t_value, color = "real parameter value"), linetype = 'dashed') +
    geom_pointinterval() +
    facet_grid(~ .variable) +
    labs(
      title = "True value vs. model estimate",
      y = "Estimate",
      x = element_blank()) +
    scale_color_manual(name = "Model", values = c(`real parameter value` = 'black', all = 'darkblue', pub = 'darkgreen'), labels = c('real parameter value', 'all studies', 'published studies')) +
    theme_minimal() +
    theme(axis.text.x = element_blank())
```
### Conclusions - the effect of publication bias:
  Introducing publication bias resulted in higher estimated mean of the intercept of the effect size, and lower estimated sd of the intercept of the effect size. However, the confidence intervals of the two models considerably overlap for both estimated parameters. In a NHST type approach the difference between the estimates would be considered non-significant. 
On the other hand, the estimates of the published studies only model  do not include the true parameter values, while the model fitted on all of the studies does.  
  
  Since we only have access to published studies in the real data, we can expect the estimated mean of the intercept to be slightly higher and the standard deviation of the Intercept to be slighly lower then 
