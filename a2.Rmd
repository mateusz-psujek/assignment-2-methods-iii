---
title: "Assignment 2 - Meta-analysis of pitch in schizophrenia"
author: "Riccardo Fusaroli"
date: "16/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages(pacman)
library(pacman)
pacman::p_load(tidyverse, brms, tidybayes, conflicted, msm, readxl)

conflict_scout() #checking any possible conflicts between packages
conflict_prefer('ar', 'brms')
conflict_prefer('filter', 'dplyr')
conflict_prefer('lag', 'dplyr') #choosing the packages to prefer if conflict arises
```

# Assignment 2: meta-analysis

## Questions to be answered

1. Simulate data to setup the analysis and gain insight on the structure of the problem. Simulate one dataset of 100 studies (n of participants should follow a normal distribution with mean of 20, sd of 10, but no fewer than 10 participants), with a mean effect size of 0.4, average deviation by study of .4 and measurement error of .8. The data you get should have one row per study, with an effect size mean and standard error. Build a proper bayesian model to analyze the simulated data. Then simulate publication bias (only some of the studies you simulate are likely to be published, which?), the effect of publication bias on your estimates (re-run the model on published studies, assess the difference), and discuss what this implies for your model. remember to use at least one plot to visualize your results. 
BONUS question: do a power/precision analysis: w this kind of sample sizes (participants) how many studies would you need to acquire good precision (e.g. .1 sd in the pop level estimate)

2. What is the current evidence for distinctive vocal patterns in schizophrenia? 
Use the data from Parola et al (2020) - https://www.dropbox.com/s/0l9ur0gaabr80a8/Matrix_MetaAnalysis_Diagnosis_updated290719.xlsx?dl=0 - focusing on pitch variability (PITCH_F0SD).  Describe the data available (studies, participants). Using the model from question 1 analyze the data, visualize and report the findings: population level effect size; how well studies reflect it; influential studies, publication bias. 
BONUS question: assess the effect of task on the estimates (model comparison with baseline model)

```{r}
# arabic digits - simulation
# ---> model fitting
# 1. Ground truth level: the real underlying truth  - norm(0.4,0.4)
# 2. Sample level(Study level: 100 samples(studies) - norm(mu drawn from the above, sd drawn from the above)
# 3. Participant level: tnorm(20, 10, 10) participants in each study
# 4. Results level: estimates of regressions based on the participants
#        -->(I) Fit no. 1 Bayes model: Given the study results what are the estimated true parameters???
# 5. Published results level: include publication bias filter
#        -->(II) Fit no. 2 Bayes model: Given the **publilshed** results what are the estimated true parameters???
#       --> Compare the 2 models


# Questions:
#   - what happens with sigma? (divergance plots)?
#   - what does 'nr' or "\r\n \r\n" in education_sz__sd mean?
#   - do NAs in healthy condition mean that there was no information or it was somehow lost or that it was an experiment that only included schizophrenic participants

#to_do:
#     - SCALE PITCH_F0SD SO IT'S SAME SCALE AS SIMULATION (cohen's d?)
#     - really dumb you have to check stuff like that xd but check whether it really means significance
#     - decide what do do about random effects (estimates for each study), should we plot them and if so how


#     - (done) does he model the pitch for 2 healthy and schizophrenic participants (how if only one distribution) or just the difference (how if you have to model individual participants as well)?
          #answer: the slides show he just models the difference
#     - (optional) maybe it would be nice to put the type of the bias and the values as function arguments
#     - (optional in part 1) **check what would removing the outliers do to model estimates!**
#   


```
## Question 1

```{r}
# arabic digits - simulation
# ---> model fitting
# 1. Ground truth level: the real underlying truth  - norm(0.4,0.4)
# 2. Sample level(Study level: 100 samples(studies) - norm(mu drawn from the above, sd drawn from the above)
# 3. Participant level: tnorm(20, 10, 10) participants in each study
# 4. Results level: estimates of regressions based on the participants
#        -->(I) Fit no. 1 Bayes model: Given the study results what are the estimated true parameters???
# 5. Published results level: include publication bias filter
#        -->(II) Fit no. 2 Bayes model: Given the **publilshed** results what are the estimated true parameters???
#       --> Compare the 2 models


# Questions:
#   - what happens with sigma? (divergance plots)?
#   - what does 'nr' or "\r\n \r\n" in education_sz__sd mean?

#to_do:
#     - (done) does he model the pitch for 2 healthy and schizophrenic participants (how if only one distribution) or just the difference (how if you have to model individual participants as well)?
          #answer: the slides show he just models the difference
#     - really dumb you have to check stuff like that xd but check whether it really means significance
#     - (optional) maybe it would be nice to put the type of the bias and the values as function arguments
#     - **check what would removing the outliers do to model estimates!**


```
```{r}
#(done) 1. Simulate data 
#     to setup the analysis and gain insight on the structure of the problem. Simulate         one dataset of 100 studies (n of participants should follow a normal distribution        with mean of 20, sd of 10, but no fewer than 10 participants), with a mean effect        size of 0.4, average deviation by study of .4 and measurement error of .8. The data       you get should have one row per study, with an effect size mean and standard error.
#      Then simulate publication bias (only some of the studies you simulate are likely #        to be published, which?)
# (done) 2.Build a proper bayesian model to analyze the simulated data.
# (done) 3. re-run the model on published studies, assess the difference, 
# 4. discuss what this implies for your model
# (done) 5. remember to use at least one plot to visualize your results. 
#6. BONUS question: do a power/precision analysis: w this kind of sample sizes ( participants) how many studies would you need to acquire good precision (e.g. .1 sd in the pop level estimate)
```
```{r}
simulate_data <- function(n = 100, gt_mean = 0.4, gt_sd = 0.4, error = 0.8, seed = 1)
  #expaning the names: n - number of studies, gt stands for 'ground-truth'
  {
  set.seed(seed)
    
  data <- tibble(study = seq(1, n, by = 1)) %>% 
    rowwise %>% 
    mutate(t_effect = rnorm(1, gt_mean, gt_sd) %>% round(2), 
           n_participants = rtnorm(1, 20, 10, lower = 10) %>% round,
           ind_effects = list(rnorm(n_participants, mean = t_effect, sd = error) %>% round(2)),
           effect = mean(ind_effects),
           effect_sigma = sd(ind_effects) / sqrt(n_participants), 
           ci_lower = effect - 1.96 * effect_sigma, # just so it's easier to plot later
           ci_upper = effect + 1.96 * effect_sigma, 
           signif = if_else(abs(effect) - 1.96 * effect_sigma > 0, 'yes' , 'no'),
           pub = if_else(signif == 'yes' & effect > 0, rbinom(1, 1, 0.9), rbinom(1, 1, 0.1))) %>% 
    ungroup %>% 
    relocate(c(t_effect, n_participants, ind_effects), .after = pub)

}
data <- simulate_data()  
```
### Checking if all worked fine
```{r}
check <- simulate_data(n = 10000)

check %>% summarise(mean(t_effect), sd(t_effect))



n_tot <- check %>%
  count(signif == 'yes' & effect > 0) %>%
  pull(n)
check %>% filter(pub == 1) %>%
  count(signif == 'yes' & effect > 0, pub) %>%
  mutate(pct = n / n_tot)

rm(n_tot)
```


```{r}

effect_hist <- function(data){
  ggplot(data, aes(x = effect)) +
    geom_histogram(binwidth = 0.1, fill = 'darkgreen', color = 'black', alpha = 0.1) +
    geom_vline(aes(xintercept = 0.4, color = 'real effect size'),
               linetype = 'dashed', size = 0.6) +
    geom_vline(aes(xintercept = mean(effect), color = 'calculated mean effect size'),
               linetype = 'dashed', size = 0.6) +
    scale_x_continuous(n.breaks = 8) +
    labs(x = expression(paste('Effect', ' ', mu)),
         y = 'Count') +
    scale_color_manual(name = element_blank(), values = c(`real effect size` = "black", `calculated mean effect size` = "darkgreen")) +
    theme_minimal()
}

effect_hist(check) + ggtitle("All studies")
effect_hist(check %>% filter(pub == 1)) + ggtitle("Published studies")

```


```{r}
rbind(check %>% mutate(index = 'all'),
      check %>% filter(pub == 1) %>% mutate(index = 'published')) %>% 
ggplot(aes(x = index, y = effect, fill = index)) + 
  geom_boxplot(alpha = 0.3) +
  theme_minimal() +
  guides(alpha = 'none') +
  scale_fill_manual(values = c('navy', 'darkgreen'), 
                    labels = c('all studies', 'published studies'), 
                    name = element_blank()) +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'top')




rm(check)
```
```{r}
f <- bf(effect | se(effect_sigma) ~ 1 + (1|study))

get_prior(f, data)

```
```{r}
priors <- c(prior(normal(0, 0.5), class = Intercept),
            prior(normal(0, 0.6), class = sd))
```
```{r}
prior_m_all <- brm(f,
                   data,
                   family = gaussian,
                   prior = priors,
                   sample_prior = 'only',
                   backend = 'cmdstanr',
                   cores = 3
                   )

```

```{r}
pp_check(prior_m_all, ndraws = 100)

summary(prior_m_all)
```
```{r}
m_all <- brm(f, 
             data,
             family = gaussian,
             prior = priors,
             sample_prior = T,
             backend = 'cmdstanr',
             cores = 3
             )

m_pub <- update(m_all, 
                newdata = data %>% filter(pub == 1))
```
```{r}
pp_check(m_all, ndraws = 100)+
  ggtitle("Model trained on all of the studies")

pp_check(m_pub, ndraws = 100)+
  ggtitle("Model trained on published studies")
```
```{r}
summary(m_all)
summary(m_pub)
```

### Convergance tests
```{r}
models <- list(m_all, m_pub)
      
# launch_shinystan(f_m) # - very nice for exploring and diagnosing the model, but opens up in a new window

map(.x = models, ~ mcmc_plot(.x, type = 'trace') + 
    theme_classic() + 
    scale_color_manual(values=c("#E66101", "#998EC3", "#542788", "#F1A340")) + 
    ylab("") + 
    xlab("Iteration") + 
    labs(subtitle = 'Trace Plots'))

map(.x = models, ~ mcmc_plot(.x, type = 'rhat_hist'))
map(.x = models, ~ mcmc_plot(.x, type = 'neff'))
```
### Posterior-prior checks

```{r}
get_variables(m_all)


```

```{r}
pp_update_plot <- function(model){

bind_rows(
  gather_draws(model, c(b_Intercept, sd_study__Intercept)) %>%
    mutate(index = 'posterior') %>% 
    mutate(.variable = if_else(.variable == 'b_Intercept', 'Intercept', 'SD')),
  gather_draws(model, `prior.*`, regex = T) %>% 
    mutate(index = 'prior') %>% 
    mutate(.variable = if_else(.variable == 'prior_Intercept', 'Intercept', 'SD'))
  ) %>%
    ggplot(aes(x = .value, fill = index, alpha = 0.3)) +
      geom_density() +
    facet_grid(~ .variable) +
    theme_minimal() +
    guides(alpha = 'none') +
    scale_fill_manual(name = element_blank(), values = c('red', 'steelblue')) +
    labs(x = element_blank())
} 

pp_update_plot(m_all)
pp_update_plot(m_pub)
```
### Comparing estimated and true values:


```{r}
bind_rows(gather_draws(m_all, c(b_Intercept, sd_study__Intercept)) %>% mean_qi, 
      gather_draws(m_pub, c(b_Intercept, sd_study__Intercept)) %>% mean_qi) %>% 
  mutate(t_value = rep(c(0.4, 0.4), times = 2),
         model = rep(c("all", "pub"), each = 2)) %>% 
  
  ggplot(aes(x = model, y = .value, ymin = .lower, ymax = .upper, color = model)) +
    geom_hline(aes(yintercept = t_value, color = "real parameter value"), linetype = 'dashed') +
    geom_pointinterval() +
    facet_grid(~ .variable) +
    labs(
      title = "True value vs. model estimate",
      y = "Estimate",
      x = element_blank()) +
    scale_color_manual(name = "Model", values = c(`real parameter value` = 'black', all = 'darkblue', pub = 'darkgreen'), labels = c('real parameter value', 'all studies', 'published studies')) +
    theme_minimal() +
    theme(axis.text.x = element_blank())
```
### Conclusions - the effect of publication bias:
  Introducing publication bias resulted in higher estimated mean of the intercept of the effect size, and lower estimated sd of the intercept of the effect size. However, the confidence intervals of the two models considerably overlap for both estimated parameters. In a NHST type approach the difference between the estimates would be considered non-significant. 
On the other hand, the estimates of the published studies only model  do not include the true parameter values, while the model fitted on all of the studies does.  
  
  Since we only have access to published studies in the real data, we can expect the estimated mean of the intercept to be slightly higher and the standard deviation of the Intercept to be slighly lower then 





## Question 2
```{r}
#What is the current evidence for distinctive vocal patterns in schizophrenia? 
#       - focusing on pitch variability (PITCH_F0SD). 


# 1. Describe the data available (studies, participants). 
# 2. Fit the models
# 3. visualize and report the findings: 
    # 3.1 population level effect size;
    # 3.2 how well studies reflect it; 
    # 3.3 influential studies, 
    # 3.4 publication bias. 
# BONUS question: assess the effect of task on the estimates (model comparison with baseline model)
```

```{r}
# saving the object from part 1. In case we have to use them we won't need to rerun the whole thing
save.image('a2_part1.Rdata') 

rm(list = setdiff(ls(), lsf.str())) # clearing the whole environment except the functions
rm(effect_hist)

data_raw <- read_excel('Matrix_MetaAnalysis_Diagnosis_updated290719.xlsx')

glimpse(data_raw)
head(data_raw)



# Selecting the relevant variables and transforming the dataframe to long format
after <- data_raw %>% 
  select(1:2 | TYPE_OF_TASK | 9:22 | starts_with('PITCH_F0SD')) %>%
  filter(!(is.na(PITCH_F0SD_HC_M) & is.na(PITCH_F0SD_SZ_M))) %>% 
  rename_with(~ str_to_lower(.x) %>% 
                str_replace_all(c('_sz_sd' = '_sd_sz', '_sz_m' = '_m_sz', 
                                '_hc_sd' = '_sd_hc', '_hc_m' = '_m_hc')) %>% 
                str_replace(fixed('_hc'), '__hc') %>% 
                str_replace(fixed('_sz'), '__sz')
              ) %>%  #this makes pivot_longer() (later in the pipeline) easier
  mutate_all(~ str_to_lower(.x) %>%  
               na_if('nr')) %>%
  add_count(studyid) %>% 
  mutate(studyid = case_when(
                     n == 1                           ~ studyid,
                     n == 2 & lag(studyid) == studyid ~ paste0(studyid, 'b'),
                     TRUE                             ~ paste0(studyid, 'a')
                     ), # dealing with repeated studyids
         n = NULL ) %>%  #deleting the now useless column created by add_count(studyid)
  pivot_longer(cols = !1:3, 
               names_to = c('.value', 'diagnosis'),
               names_sep= '__') %>% 
  mutate(across(1:4, as_factor),
         across(!1:3, ~ str_replace_all(.x, ',', '.') %>% str_remove_all("[^0-9.]") %>% as.numeric)) %>% #there were weird cells like '2,63197\r\n \r\n	' that needed to be fixed before converting to numeric)
  rename('n_diagnosis' = 'sample_size', ) %>% 
      #   'effect' = 'pitch_f0sd_m',
       #  'effect_sigma' = 'pitch_f0sd_sd')
  group_by(studyid) %>% 
  mutate(sample_size = if_else(TRUE %in% is.na(n_diagnosis),
                               n_diagnosis[!is.na(n_diagnosis)][[1]],
                               sum(n_diagnosis)),
         .after = n_diagnosis) %>% 
  ungroup
  
head(data)
```
###### Note:
We weren't really sure whether NA's in some of the 'HC'conditions rows meant that the information about that condition was unavailable or somehow lost or rather just that the experiment didn't have a condition with only healthy participants. We assumed the second option, so the total sample size in such cases just equals to the sample size of the 'SZ' conditon.
##### Sample size
```{r}
# Mean total sample size
data %>% filter(diagnosis == 'sz') %>%
  ggplot(aes(x = sample_size)) +
    geom_histogram(fill = 'brown', color = 'black', alpha = 0.4) +
    geom_vline(aes(xintercept = mean(sample_size, na.rm = T), color = 'mean sample size'),
                 linetype = 'dashed', size = 0.6) +
    theme_minimal() +
    labs(title = "Total sample size across the dataset",
         x = "Sample size \n (total across conditions)",
         y = "Number of studies") +
    scale_x_continuous(n.breaks = 10) +
    scale_color_manual(name = element_blank(), values = c(`mean sample size` = 'brown'))


#where the conditions(if present) balanced in terms of size?
data %>%
  ggplot(aes(x = studyid, y = n_diagnosis, fill = diagnosis)) +
    geom_bar(stat = 'identity', alpha = 0.9) +
    geom_text(aes(label = n_diagnosis), 
              size = 2.5, 
              position = position_stack(vjust = 0.3)) +
    theme_minimal() +
      labs(title = "Number of participants in each condition by study",
           x = "Study",
           y = "Number of participants") +
      scale_fill_manual(name = element_blank(), 
                        labels = c('Schizophernic condition', 'Control condition'), 
                        values = c('steelblue', 'darkolivegreen')) +
      theme(axis.text.x = element_text(angle = 90))

data %>% 
  group_by(studyid, diagnosis) %>% 
  summarise(n_diagnosis = n_diagnosis, sample_size = sample_size, pct = n_diagnosis / sample_size) %>% 
  ggplot(aes(x = studyid, fill = diagnosis)) +
    geom_bar(aes(y = pct), stat = 'identity', alpha = 0.9) +
    geom_hline(aes(yintercept = 0.5), linetype = 'dashed', size = 0.5) +
    theme_minimal() +
    labs(title = "Proportion of the two conditions in the total sample size by study",
         x = "Study",
         y = "Proportion") +
    scale_fill_manual(name = element_blank(), 
                      labels = c('Schizophrenic condition', 'Control condition'), 
                      values = c('steelblue', 'darkolivegreen')) +
    theme(axis.text.x = element_text(angle = 90))



#sum of the samples of each condition
data %>% 
  group_by(diagnosis) %>% 
  summarise(n = sum(n_diagnosis, na.rm = T)) %>% mutate(pct = n / sum(n))
```

##### Age
```{r}
# I split them in two dfs instead of making one, because I realised that makes each value repeat 2, which then makes the plot thicker

data %>% 
  select(studyid, diagnosis, age_m, age_sd) %>% 
  pivot_longer(c(age_m, age_sd),
               names_to = c(".value","age_parameter"),
               names_sep = "_") %>% 
              mutate(age_parameter = ifelse(age_parameter =="m", "Age Mean", "Age SD")) %>%                            
        ggplot(aes(x = diagnosis, y = age, fill = diagnosis))+
          geom_violin()+
          geom_boxplot(width = 0.05, fill = "white") +
          facet_wrap(~age_parameter) +
          labs(y = NULL) +
          theme_minimal()



```
##### Education
```{r}
data %>% 
  select(studyid, diagnosis, education_m, education_sd) %>% 
  pivot_longer(c(education_m, education_sd),
               names_to = c(".value","edu_parameter"),
               names_sep = "_") %>% 
  mutate(edu_parameter = ifelse(edu_parameter == "m",
                                "Years of Education Mean",
                                "Years of Education SD")) %>%
  ggplot(aes(x = diagnosis, y = education, fill = diagnosis)) +
    geom_violin() +
    geom_boxplot(width = 0.05, fill = "white") +
    facet_wrap(~edu_parameter)+
    labs(y = NULL) +
    theme_minimal()

```
### Influential studies
```{r}

ggplot(data, aes(y = effect)) +
  geom_boxplot()



```
### Fitting the model
#### Defining the formula
```{r}
f <- bf(effect | se(effect_sigma) ~ 1 + (1|study))
```
#### Prior only
```{r}
get_prior(f, data)

priors <- c(prior(normal(0, 0.5), class = Intercept),
            prior(normal(0, 0.6), class = sd))

```

```{r}
prior_m <- brm(f,
                   data,
                   family = gaussian,
                   prior = priors,
                   sample_prior = 'only',
                   backend = 'cmdstanr',
                   cores = 3
                   )
```

```{r}
pp_check(prior_m, ndraws = 100)
```

```{r}
m <- brm(f, 
         data,
         family = gaussian,
         prior = priors,
         sample_prior = 'only',
         backend = 'cmdstanr',
         cores = 3
                   )
```
```{r}
pp_check(m, ndraws = 100)
```

